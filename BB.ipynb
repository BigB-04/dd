{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d4ce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 : Frequency of a given word in the given file\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "\n",
    "sc=spark.sparkContext\n",
    "\n",
    "# Read the input file and Calculating words count\n",
    "text_file = sc.textFile(\"input.txt\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "                            .map(lambda word: (word, 1)) \\\n",
    "                           .reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "output = counts.collect()\n",
    "for (word, count) in output:\n",
    "    print(\"%s: %i\" % (word, count))\n",
    "    \n",
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c379b37f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4962a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 : Matrix Multiplication\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MatrixMultiplication\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample matrices\n",
    "matrix1 = [\n",
    "    (0, 0, 2),\n",
    "    (0, 1, 3),\n",
    "    (1, 0, 4),\n",
    "    (1, 1, 5)\n",
    "]\n",
    "\n",
    "matrix2 = [\n",
    "    (0, 0, 6),\n",
    "    (0, 1, 7),\n",
    "    (1, 0, 8),\n",
    "    (1, 1, 9)\n",
    "]\n",
    "\n",
    "# Create RDDs from the matrices\n",
    "matrix1_rdd = spark.sparkContext.parallelize(matrix1)\n",
    "matrix2_rdd = spark.sparkContext.parallelize(matrix2)\n",
    "\n",
    "# Perform matrix multiplication using map-reduce\n",
    "result_rdd = matrix1_rdd.flatMap(lambda x: [((x[0], y[1]), x[2] * y[2]) for y in matrix2 if x[1] == y[0]]). \\\n",
    "    reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "\n",
    "# Convert RDD to DataFrame\n",
    "result_df = spark.createDataFrame(result_rdd.map(lambda x: (x[0][0], x[0][1], x[1])), [\"row\", \"col\", \"result\"])\n",
    "\n",
    "# Display the result\n",
    "result_df.show()\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2a22d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6199d49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 : Grades of Students\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StudentGrades\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample student scores\n",
    "scores = [\n",
    "    (\"Alice\", {\"Math\": 85, \"Science\": 90, \"English\": 80}),\n",
    "    (\"Bob\", {\"Math\": 70, \"Science\": 75, \"English\": 85}),\n",
    "    (\"Charlie\", {\"Math\": 60, \"Science\": 65, \"English\": 70}),\n",
    "    (\"David\", {\"Math\": 90, \"Science\": 95, \"English\": 85}),\n",
    "    (\"Eve\", {\"Math\": 75, \"Science\": 80, \"English\": 75})\n",
    "]\n",
    "\n",
    "# Create RDD from the scores\n",
    "scores_rdd = spark.sparkContext.parallelize(scores)\n",
    "\n",
    "# Define the grading scheme (example)\n",
    "grading_scheme = {\n",
    "    \"A\": (80, 100),\n",
    "    \"B\": (60, 79),\n",
    "    \"C\": (40, 59),\n",
    "    \"D\": (0, 39)\n",
    "}\n",
    "\n",
    "# Function to compute grades for a given score\n",
    "def compute_grade(score):\n",
    "    for grade, (lower_bound, upper_bound) in grading_scheme.items():\n",
    "        if lower_bound <= score <= upper_bound:\n",
    "            return grade\n",
    "    return \"F\"\n",
    "\n",
    "# Map operation to compute grades for each student\n",
    "grades_rdd = scores_rdd.map(lambda x: (x[0], {subject: compute_grade(score) for subject, score in x[1].items()}))\n",
    "\n",
    "# Convert RDD to DataFrame\n",
    "grades_df = spark.createDataFrame(grades_rdd.flatMap(lambda x: [(x[0], subject, grade) for subject, grade in x[1].items()]), [\"Student\", \"Subject\", \"Grade\"])\n",
    "\n",
    "# Display the result\n",
    "grades_df.show()\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ee6748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a097a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 : Titanic Ship Data\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TitanicAnalysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the Titanic dataset\n",
    "titanic_df = spark.createDataFrame(sns.load_dataset(\"titanic\"))\n",
    "\n",
    "titanic_df = titanic_df.fillna({'Age': 0})\n",
    "\n",
    "# Filter data for male passengers who died and remove null values from Age column\n",
    "male_deceased = titanic_df.filter((titanic_df[\"Sex\"] == \"male\") & (titanic_df[\"Survived\"] == 0) & titanic_df[\"Age\"].isNotNull())\n",
    "\n",
    "# Check if there are any male passengers who died\n",
    "male_deceased_count = male_deceased.count()\n",
    "\n",
    "if male_deceased_count > 0:\n",
    "    # Calculate the average age of male passengers who died\n",
    "# Calculate the average age of male passengers who died\n",
    "    male_deceased_age_avg = male_deceased.agg({\"Age\": \"avg\"}).collect()[0][0]\n",
    "    print(\"Number of male passengers who died:\", male_deceased_count)\n",
    "    print(\"Average age of male passengers who died:\", male_deceased_age_avg)\n",
    "else:\n",
    "    print(\"No male passengers found who died in the dataset.\")\n",
    "    \n",
    "female_deceased_by_class = titanic_df.filter((titanic_df[\"Sex\"] == \"female\") & (titanic_df[\"Survived\"] == 0)).groupBy(\"Pclass\").count()\n",
    "\n",
    "# Display results\n",
    "print(\"Number of deceased passengers in each class among females:\")\n",
    "female_deceased_by_class.show()\n",
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe39f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faefcdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11 : Mongo DB\n",
    "\n",
    "# All this have to perform on Mongo DB from terminal\n",
    "\n",
    "sudo systemctl status mongodb\n",
    "sudo systemctl start mongodb\n",
    "mongodb\n",
    "\n",
    "use report_db       // use & create DB\n",
    "db                  //to check the currently selected bd\n",
    "show dbs            //  list of all databases in MongoDB\n",
    "\n",
    "//create a collection (table)\n",
    "db.createCollection(\"report\") // another method to create collection \n",
    "db.report.insertOne({name: \"akasmit\", age: 22, gender: \"Male\"})            //db.table_name.insert() OR db.table_name.insertOne()\n",
    "db.report.insertMany([{name: \"ram\", age:25, gender:\"Male\"},\n",
    "              {name:\"rohan\", age:20, gender:\"Male\"},\n",
    "              {name:\"shweta\", age:21, gender:\"Female\"}])\n",
    "\n",
    "db.report.find().pretty()        //display all documents in the collection\n",
    "db.report.find({}, {\"report_db.report.name\":1, \"report_db.report.age\":1})     // 1 to show and 2 not to show\n",
    "\n",
    "db.report.updateOne({name: \"akasmit\"}, {$set:{age:21}})\n",
    "db.report.updateMany({age:21}, {$set:{eligible:\"true\"}})\n",
    "OR\n",
    "db.UpdateMany({}, {&set:{year:2024}})                     //will add an year column\n",
    "\n",
    "\n",
    "db.report.deleteOne({name:\"rohan\"})\n",
    "db.table_name.deleteMany({})                        //ye dekhna padega\n",
    "\n",
    "d.report.save()\n",
    "\n",
    "//Logical Operators\n",
    "db.report.find({$and: [{name:\"akasmit\"}, {gender:\"male\"}] }).pretty()\n",
    "db.report.find({$or: [{name:\"ram\"}, {age:25}]})\n",
    "db.report.find({$not: [{name:\"shweta\"}, {age:21}]})             //dekhna padega\n",
    "db.report.find({$nor: [{name:\"shweta\"}, {age:21}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e9c5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8240224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PowerBI  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a8d65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 : Classification Algorithm\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import precision_score,confusion_matrix,accuracy_score,recall_score, classification_report\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"Iris.csv\")\n",
    "df.head()\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "df[\"Species\"]=encoder.fit_transform(df[\"Species\"])\n",
    "df\n",
    "\n",
    "\n",
    "x = df.iloc[:, [1, 4]].values\n",
    "y = df.iloc[:, 5].values\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "classifier = GaussianNB()    \n",
    "classifier.fit(xtrain, ytrain) \n",
    "\n",
    "ypred=classifier.predict(xtest)\n",
    "\n",
    "confusion=confusion_matrix(ytest,ypred)\n",
    "confusion\n",
    "\n",
    "accuracy=accuracy_score(ytest,ypred)\n",
    "print(accuracy)\n",
    "\n",
    "precision=precision_score(ytest,ypred,average='micro')\n",
    "print(precision)\n",
    "\n",
    "recall=recall_score(ytest,ypred,average='micro')\n",
    "recall\n",
    "\n",
    "cl_report = classification_report(ytest,ypred)\n",
    "print(cl_report) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22436c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38e0704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 : Clustering Algorithm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "data = pd.read_csv(\"/content/ML4-Iris.csv\")\n",
    "X = data.iloc[:, 1:5].values  # Selecting the features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)\n",
    "\n",
    "#elbow method\n",
    "wcss = []  # Within-cluster sum of squares\n",
    "for i in range(1, 15):\n",
    "    kmeans = KMeans(n_clusters=i, n_init=10)\n",
    "    kmeans.fit(X_std)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1, 15), wcss, '-bx')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "#K-means\n",
    "kmeans = KMeans(n_clusters = 2)\n",
    "kmeans.fit(X_std)\n",
    "\n",
    "print(kmeans.cluster_centers_)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "plt.scatter(data['SepalLengthCm'], data['SepalWidthCm'],c=kmeans.labels_)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
